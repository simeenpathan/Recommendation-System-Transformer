{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqavsZoXU/LpfRvAOcEvuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simeenpathan/Recommendation-System-Transformer/blob/main/Transformer_Based_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First cell: Import necessary libraries\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras  # Remove this line if using keras module exclusively\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from tensorflow.keras import layers  # Remove this line if using keras module exclusively\n",
        "# from tensorflow.keras.layers import StringLookup  # Remove this line if using keras module exclusively\n",
        "\n",
        "# Second cell: Example code to demonstrate the setup\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "# print(\"Keras version:\", keras.__version__)  # Remove this line if using keras module exclusively"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs6huu8X_vyq",
        "outputId": "c9b3df1b-a690-4afd-f607-ae86757d92ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "k-xMa_Wc_9-A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
        "zip_file = \"movielens.zip\"\n",
        "urlretrieve(url, zip_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-60Aka8AEcs",
        "outputId": "542ceda9-7430-45ca-f315-493c1628e383"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('movielens.zip', <http.client.HTTPMessage at 0x7c82e77dc430>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "4j4cgbxdAJV8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osUlHObVAPnR",
        "outputId": "bedf2696-b913-4aec-cc49-16a3f9641b96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'ml-1m', 'movielens.zip', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(\n",
        "    \"ml-1m/users.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")\n",
        "\n",
        "ratings = pd.read_csv(\n",
        "    \"ml-1m/ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    \"ml-1m/movies.dat\",\n",
        "    sep=\"::\",\n",
        "    names=[\"movie_id\", \"title\", \"genres\"],\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    engine=\"python\",\n",
        ")"
      ],
      "metadata": {
        "id": "Z0kl4LBAASKj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
        "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
        "\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))"
      ],
      "metadata": {
        "id": "eYV5kQbXAZOQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\"]\n",
        "genres += [\"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\"]\n",
        "genres += [\"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
        "\n",
        "for genre in genres:\n",
        "    movies[genre] = movies[\"genres\"].apply(\n",
        "        lambda values: int(genre in values.split(\"|\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "47tN54tBAia0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
        "\n",
        "ratings_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),\n",
        "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
        "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
        "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ETrdYA3CAmNj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "step_size = 2\n",
        "\n",
        "\n",
        "def create_sequences(values, window_size, step_size):\n",
        "    sequences = []\n",
        "    start_index = 0\n",
        "    while True:\n",
        "        end_index = start_index + window_size\n",
        "        seq = values[start_index:end_index]\n",
        "        if len(seq) < window_size:\n",
        "            seq = values[-window_size:]\n",
        "            if len(seq) == window_size:\n",
        "                sequences.append(seq)\n",
        "            break\n",
        "        sequences.append(seq)\n",
        "        start_index += step_size\n",
        "    return sequences\n",
        "\n",
        "\n",
        "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "ratings_data.ratings = ratings_data.ratings.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "del ratings_data[\"timestamps\"]"
      ],
      "metadata": {
        "id": "-UEcRTwlAtL-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\n",
        "    \"movie_ids\", ignore_index=True\n",
        ")\n",
        "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
        "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
        "ratings_data_transformed = ratings_data_transformed.join(\n",
        "    users.set_index(\"user_id\"), on=\"user_id\"\n",
        ")\n",
        "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(\n",
        "    lambda x: \",\".join(x)\n",
        ")\n",
        "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
        "    lambda x: \",\".join([str(v) for v in x])\n",
        ")\n",
        "\n",
        "del ratings_data_transformed[\"zip_code\"]\n",
        "\n",
        "ratings_data_transformed.rename(\n",
        "    columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"},\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "f1VWiTwXAy9y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
        "train_data = ratings_data_transformed[random_selection]\n",
        "test_data = ratings_data_transformed[~random_selection]\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)"
      ],
      "metadata": {
        "id": "nfuv-TGlA4N5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER = list(ratings_data_transformed.columns)\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"user_id\": list(users.user_id.unique()),\n",
        "    \"movie_id\": list(movies.movie_id.unique()),\n",
        "    \"sex\": list(users.sex.unique()),\n",
        "    \"age_group\": list(users.age_group.unique()),\n",
        "    \"occupation\": list(users.occupation.unique()),\n",
        "}\n",
        "\n",
        "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
        "\n",
        "MOVIE_FEATURES = [\"genres\"]"
      ],
      "metadata": {
        "id": "BsbFpA-dA9ft"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    def process(features):\n",
        "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
        "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
        "\n",
        "        # The last movie id in the sequence is the target movie.\n",
        "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
        "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
        "\n",
        "        ratings_string = features[\"sequence_ratings\"]\n",
        "        sequence_ratings = tf.strings.to_number(\n",
        "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
        "        ).to_tensor()\n",
        "\n",
        "        # The last rating in the sequence is the target for the model to predict.\n",
        "        target = sequence_ratings[:, -1]\n",
        "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
        "\n",
        "        return features, target\n",
        "\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        field_delim=\"|\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(process)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "DPSsKjnMBByv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
        "        \"sequence_movie_ids\": keras.Input(\n",
        "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"target_movie_id\": keras.Input(\n",
        "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
        "        ),\n",
        "        \"sequence_ratings\": keras.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
        "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
        "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
        "    }"
      ],
      "metadata": {
        "id": "clATiXCYBJdI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        movie_idx = movie_index_lookup(movie_id)\n",
        "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
        "        encoded_movie = movie_embedding\n",
        "        if include_movie_features:\n",
        "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
        "            encoded_movie = movie_embedding_processor(\n",
        "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
        "            )\n",
        "        return encoded_movie\n",
        "\n",
        "    ## Encoding target_movie_id\n",
        "    target_movie_id = inputs[\"target_movie_id\"]\n",
        "    encoded_target_movie = encode_movie(target_movie_id)\n",
        "\n",
        "    ## Encoding sequence movie_ids.\n",
        "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
        "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
        "    # Create positional embedding.\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encodded_positions = position_embedding_encoder(positions)\n",
        "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
        "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
        "    sequence_ratings = keras.ops.expand_dims(sequence_ratings, -1)\n",
        "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
        "    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs.\n",
        "    for i in range(sequence_length - 1):\n",
        "        feature = encoded_sequence_movies_with_poistion_and_rating[:, i, ...]\n",
        "        feature = keras.ops.expand_dims(feature, 1)\n",
        "        encoded_transformer_features.append(feature)\n",
        "    encoded_transformer_features.append(encoded_target_movie)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features"
      ],
      "metadata": {
        "id": "9_d-wPCbBNys"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow import keras\n",
        "\n",
        "# Placeholder for creating model inputs\n",
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": tf.keras.Input(name=\"user_id\", shape=(), dtype=tf.int32),\n",
        "        \"movie_id\": tf.keras.Input(name=\"movie_id\", shape=(), dtype=tf.int32)\n",
        "    }\n",
        "\n",
        "# Placeholder for encoding input features\n",
        "def encode_input_features(inputs, include_user_id, include_user_features, include_movie_features):\n",
        "    # Embedding for user_id\n",
        "    user_embedding = layers.Embedding(input_dim=10000, output_dim=64)(inputs[\"user_id\"])\n",
        "    user_embedding = layers.Reshape((1, -1))(user_embedding)  # Shape (batch_size, 1, 64)\n",
        "\n",
        "    # Optionally include movie features\n",
        "    other_features = None\n",
        "    if include_user_features or include_movie_features:\n",
        "        movie_embedding = layers.Embedding(input_dim=10000, output_dim=64)(inputs[\"movie_id\"])\n",
        "        movie_embedding = layers.Reshape((1, -1))(movie_embedding)  # Shape (batch_size, 1, 64)\n",
        "        other_features = movie_embedding\n",
        "\n",
        "    return user_embedding, other_features\n",
        "\n",
        "# Parameters\n",
        "include_user_id = False\n",
        "include_user_features = False\n",
        "include_movie_features = False\n",
        "\n",
        "hidden_units = [256, 128]\n",
        "dropout_rate = 0.1\n",
        "num_heads = 3\n",
        "\n",
        "# Model creation function\n",
        "def create_model():\n",
        "    inputs = create_model_inputs()\n",
        "    transformer_features, other_features = encode_input_features(\n",
        "        inputs, include_user_id, include_user_features, include_movie_features\n",
        "    )\n",
        "\n",
        "    # Create a multi-headed attention layer.\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=transformer_features.shape[-1], dropout=dropout_rate\n",
        "    )(transformer_features, transformer_features)\n",
        "\n",
        "    # Transformer block.\n",
        "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
        "    x1 = layers.Add()([transformer_features, attention_output])\n",
        "    x1 = layers.LayerNormalization()(x1)\n",
        "    x2 = layers.LeakyReLU()(x1)\n",
        "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
        "    x2 = layers.Dropout(dropout_rate)(x2)\n",
        "    transformer_features = layers.Add()([x1, x2])\n",
        "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
        "    features = layers.Flatten()(transformer_features)\n",
        "\n",
        "    # Include other features if any\n",
        "    if other_features is not None:\n",
        "        features = layers.concatenate([features, layers.Flatten()(other_features)])\n",
        "\n",
        "    # Fully-connected layers.\n",
        "    for num_units in hidden_units:\n",
        "        features = layers.Dense(num_units)(features)\n",
        "        features = layers.BatchNormalization()(features)\n",
        "        features = layers.LeakyReLU()(features)\n",
        "        features = layers.Dropout(dropout_rate)(features)\n",
        "\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Print the model summary to check the structure\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMPEn3tRBWQj",
        "outputId": "7564f9a8-6d29-4acf-e5e6-c336305dca48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " user_id (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 64)                   640000    ['user_id[0][0]']             \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 1, 64)                0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 1, 64)                49792     ['reshape[0][0]',             \n",
            " iHeadAttention)                                                     'reshape[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 1, 64)                0         ['multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " add (Add)                   (None, 1, 64)                0         ['reshape[0][0]',             \n",
            "                                                                     'dropout[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 1, 64)                128       ['add[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)     (None, 1, 64)                0         ['layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1, 64)                4160      ['leaky_re_lu[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 1, 64)                0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 1, 64)                0         ['layer_normalization[0][0]', \n",
            "                                                                     'dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 1, 64)                128       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 64)                   0         ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  16640     ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 256)                  1024      ['dense_1[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 256)                  0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 256)                  0         ['leaky_re_lu_1[0][0]']       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 128)                  32896     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 128)                  512       ['dense_2[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 128)                  0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['leaky_re_lu_2[0][0]']       \n",
            "                                                                                                  \n",
            " movie_id (InputLayer)       [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1)                    129       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 745409 (2.84 MB)\n",
            "Trainable params: 744641 (2.84 MB)\n",
            "Non-trainable params: 768 (3.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": keras.Input(name=\"user_id\", shape=(), dtype=tf.int32),\n",
        "        \"movie_id\": keras.Input(name=\"movie_id\", shape=(), dtype=tf.int32)\n",
        "    }"
      ],
      "metadata": {
        "id": "JFP0v4nOBvv_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    def process(features):\n",
        "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
        "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
        "\n",
        "        # The last movie id in the sequence is the target movie.\n",
        "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
        "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
        "\n",
        "        ratings_string = features[\"sequence_ratings\"]\n",
        "        sequence_ratings = tf.strings.to_number(\n",
        "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
        "        ).to_tensor()\n",
        "\n",
        "        # The last rating in the sequence is the target for the model to predict.\n",
        "        target = sequence_ratings[:, -1]\n",
        "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
        "\n",
        "        # Include movie_id as a feature.\n",
        "        features[\"movie_id\"] = features.pop(\"movie_id\")\n",
        "\n",
        "        return features, target"
      ],
      "metadata": {
        "id": "QKyb3tTCB1AM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=True,\n",
        "    include_movie_features=True,\n",
        "):\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    if include_user_features:\n",
        "        other_feature_names.extend(USER_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    # Do not create an embedding for the user_id feature.\n",
        "    # Instead, convert it to an integer tensor.\n",
        "    if include_user_id:\n",
        "        user_id_tensor = tf.cast(inputs[\"user_id\"], tf.int32)\n",
        "        encoded_other_features.append(user_id_tensor)\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a movie embedding encoder\n",
        "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
        "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    movie_index_lookup = StringLookup(\n",
        "        vocabulary=movie_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=0,\n",
        "        name=\"movie_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    movie_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(movie_vocabulary),\n",
        "        output_dim=movie_embedding_dims,\n",
        "        name=f\"movie_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    genre_vectors = movies[genres].to_numpy()\n",
        "    movie_genres_lookup = layers.Embedding(\n",
        "        input_dim=genre_vectors.shape[0],\n",
        "        output_dim=genre_vectors.shape[1],\n",
        "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
        "        trainable=False,\n",
        "        name=\"genres_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    movie_embedding_processor = layers.Dense(\n",
        "        units=movie_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_movie_embedding_with_genres\",\n",
        "    )\n",
        "## Define a function to encode a given movie id.\n",
        "    def encode_movie(movie_id) :\n",
        "      \"\"\"\n",
        "\n",
        "      Args:\n",
        "        movie_id:\n",
        "      \"\"\""
      ],
      "metadata": {
        "id": "1r_H3irODVbK"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}